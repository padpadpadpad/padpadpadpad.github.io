<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Daniel Padfield</title>
    <link>https://padpadpadpad.github.io/categories/r/</link>
    <description>Recent content in R on Daniel Padfield</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Daniel Padfield</copyright>
    <lastBuildDate>Sun, 07 Jan 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/r/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Fitting non-linear regressions with broom, purrr and nls.multstart</title>
      <link>https://padpadpadpad.github.io/post/fitting-non-linear-regressions-with-broom-purrr-and-nls.multstart/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://padpadpadpad.github.io/post/fitting-non-linear-regressions-with-broom-purrr-and-nls.multstart/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;With my research, I often use non-linear least squares regression to fit a model with biologically meaningful parameters to data. Specifically, I measure the thermal performance of phytoplankon growth, respiration and photosynthesis over a wide range of assay temperatures to see how the organisms are adapted to the temperatures they live at.&lt;/p&gt;
&lt;p&gt;These thermal performance curves generally follow a unimodal shape and parameters for which are widely used in climate change research to predict whether organisms will be able to cope with increasing temperatures.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://padpadpadpad.github.io/img/TPC.png&#34; alt=&#34;Example Thermal Performance Curve&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Example Thermal Performance Curve&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These curves can be modelled with a variety of equations, such as the Sharpe-Schoolfield equation, which I have log-transformed here:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[log(rate) = lnc + E(\frac{1}{T_{c}} - \frac{1}{kT}) - ln(1 + e^{E_h(\frac{1}{kT_h} - \frac{1}{kT})})\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(lnc\)&lt;/span&gt; is a normalisation constant at a common temperature, &lt;span class=&#34;math inline&#34;&gt;\(T_{c}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; is an activation energy that describes the rate of increase before the optimum temperature, &lt;span class=&#34;math inline&#34;&gt;\(T_{opt}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is Boltzmann’s constant, &lt;span class=&#34;math inline&#34;&gt;\(E_{h}\)&lt;/span&gt; is the deactivation energy that controls the decline in rate past the optimum temperature and &lt;span class=&#34;math inline&#34;&gt;\(T_{h}\)&lt;/span&gt; is the temperature where, after the optimu, the rate is half of the maximal rate.&lt;/p&gt;
&lt;p&gt;Say I want to fit the same equation to 10, 50, or 100s of these curves. I could loop through a call to &lt;strong&gt;nls&lt;/strong&gt;, &lt;strong&gt;nlsLM&lt;/strong&gt;, or use &lt;strong&gt;nlsList&lt;/strong&gt; from &lt;strong&gt;nlme&lt;/strong&gt;. However, non-linear least squares regression in R is sensitive to the start parameters, meaning that different start parameters can give different “best estimated parameters”. This becomes more likely when fitting more curves with only a single set of start parameters, where the variation in estimated parameter values is likely to be much larger. For example, some curves could have much higher rates (&lt;span class=&#34;math inline&#34;&gt;\(lnc\)&lt;/span&gt;), higher optimum temperatures (i.e. &lt;span class=&#34;math inline&#34;&gt;\(T_{h}\)&lt;/span&gt;) or have different values of temperature-dependence (&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To combat this, I wrote an R package which allows for multiple start parameters for non-linear regression. I wrapped this method in an R package called &lt;a href=&#34;https://github.com/padpadpadpad/nlsLoop&#34;&gt;&lt;strong&gt;nlsLoop&lt;/strong&gt;&lt;/a&gt; and submitted it to The Journal of Open Source Software. Everything was good with the world and I went to a Christmas party.&lt;/p&gt;
&lt;p&gt;The next day, I had an epiphany surrounding the redundancies and needless complexities of my R package, withdrew my submission and rewrote the entire package in a weekend to give rise to a single function package, &lt;strong&gt;nls.multstart::nls_multstart()&lt;/strong&gt;. Essentially since I first wrote &lt;strong&gt;nlsLoop&lt;/strong&gt; ~3 years ago I have realised that &lt;strong&gt;broom&lt;/strong&gt; and &lt;strong&gt;purrr&lt;/strong&gt; can do what I wrote clunkier functions to achieve. In contrast, &lt;a href=&#34;https://github.com/padpadpadpad/nls.multstart&#34;&gt;&lt;strong&gt;nls.multstart&lt;/strong&gt;&lt;/a&gt; works perfectly with the tools of the &lt;strong&gt;tidyverse&lt;/strong&gt; to fit multiple models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-model-fitting-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple model fitting in practice&lt;/h2&gt;
&lt;p&gt;Load in all packages that are used in this analysis. Packages can be installed from GitHub using &lt;strong&gt;devtools&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(nls.multstart) # devtools::install_github(&amp;#39;padpadpadpad/nls.multstart&amp;#39;)
library(ggplot2)
library(broom)
library(purrr)
library(dplyr)
library(tidyr)
library(nlstools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then load in the data and have a look at it using &lt;strong&gt;glimpse()&lt;/strong&gt;. Here we shall use a dataset of thermal performance curves of metabolism of &lt;strong&gt;Chlorella vulgaris&lt;/strong&gt; from Padfield &lt;strong&gt;et al.&lt;/strong&gt; 2016.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load in example data set
data(&amp;quot;Chlorella_TRC&amp;quot;)

glimpse(Chlorella_TRC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 649
## Variables: 7
## $ curve_id    &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,...
## $ growth.temp &amp;lt;dbl&amp;gt; 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20...
## $ process     &amp;lt;chr&amp;gt; &amp;quot;acclimation&amp;quot;, &amp;quot;acclimation&amp;quot;, &amp;quot;acclimation&amp;quot;, &amp;quot;accl...
## $ flux        &amp;lt;chr&amp;gt; &amp;quot;respiration&amp;quot;, &amp;quot;respiration&amp;quot;, &amp;quot;respiration&amp;quot;, &amp;quot;resp...
## $ temp        &amp;lt;dbl&amp;gt; 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 16...
## $ K           &amp;lt;dbl&amp;gt; 289.15, 292.15, 295.15, 298.15, 301.15, 304.15, 30...
## $ ln.rate     &amp;lt;dbl&amp;gt; -2.06257833, -1.32437939, -0.95416807, -0.79443675...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we define the Sharpe-Schoolfield equation discussed earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the Sharpe-Schoolfield equation
schoolfield_high &amp;lt;- function(lnc, E, Eh, Th, temp, Tc) {
  Tc &amp;lt;- 273.15 + Tc
  k &amp;lt;- 8.62e-5
  boltzmann.term &amp;lt;- lnc + log(exp(E/k*(1/Tc - 1/temp)))
  inactivation.term &amp;lt;- log(1/(1 + exp(Eh/k*(1/Th - 1/temp))))
  return(boltzmann.term + inactivation.term)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 60 curves in this dataset, 30 each of photosynthesis and respiration. The treatments are growth temperature (20, 23, 27, 30, 33 ºC) and adaptive process (acclimation or adaptation) that reflects the number of generations cultures were grown at each temperature.&lt;/p&gt;
&lt;p&gt;We can see how &lt;strong&gt;nls_multstart()&lt;/strong&gt; works by subsetting the data for a single curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subset dataset
d_1 &amp;lt;- subset(Chlorella_TRC, curve_id == 1)

# run nls_multstart
fit &amp;lt;- nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                     data = d_1,
                     iter = 500,
                     param_bds = c(-10, 10, 0.1, 2, 0.5, 5, 285, 330),
                     supp_errors = &amp;#39;Y&amp;#39;,
                     AICc = &amp;#39;Y&amp;#39;,
                     na.action = na.omit,
                     lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))

fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nonlinear regression model
##   model: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)
##    data: data
##      lnc        E       Eh       Th 
##  -1.3462   0.9877   4.3326 312.1887 
##  residual sum-of-squares: 7.257
## 
## Number of iterations to convergence: 14 
## Achieved convergence tolerance: 1.49e-08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;nls_multstart()&lt;/strong&gt; allows boundaries for each parameter to be set. A uniform distribution between these values is created and start values for each iteration of the fitting process are then picked randomly. The function returns the best available model by picking the model with the lowest AIC/AICc score. Additional info on the function can be found &lt;a href=&#34;https://github.com/padpadpadpad/nls.multstart&#34;&gt;here&lt;/a&gt; or by typing &lt;code&gt;?nls_multstart&lt;/code&gt; into the R console.&lt;/p&gt;
&lt;p&gt;This fit can then be “tidied” in various ways using the R package &lt;strong&gt;broom&lt;/strong&gt;. Each different function in &lt;strong&gt;broom&lt;/strong&gt; returns a different set of information. &lt;strong&gt;tidy()&lt;/strong&gt; returns the estimated parameters, &lt;strong&gt;augment()&lt;/strong&gt; returns the predictions and &lt;strong&gt;glance()&lt;/strong&gt; returns information about the model such as the AIC score and whether the model has reached convergence. Confidence intervals of non-linear regression can also be estimated using &lt;strong&gt;nlstools::confint2()&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The amazing thing about these tools is the ease at which they can then be used on multiple curves at once, an approach Hadley Wickham has previously &lt;a href=&#34;https://blog.rstudio.com/2016/02/02/tidyr-0-4-0/&#34;&gt;written about&lt;/a&gt;. The approach nests the data based on grouping variables using &lt;strong&gt;nest()&lt;/strong&gt;, then creates a list column of the best fit for each curve using &lt;strong&gt;map()&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit over each set of groupings
fits &amp;lt;- Chlorella_TRC %&amp;gt;%
  group_by(., flux, growth.temp, process, curve_id) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(fit = purrr::map(data, ~ nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                                   data = .x,
                                   iter = 1000,
                                   param_bds = c(-100, 100, 0.1, 2, 0.5, 10, 285, 330),
                                   supp_errors = &amp;#39;Y&amp;#39;,
                                   AICc = &amp;#39;Y&amp;#39;,
                                   na.action = na.omit,
                                   lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are confused, then you are not alone. This took me a long time to understand and I imagine there are still better ways for me to do it! However, to check it has worked, we can look at a single fit to check it looks ok. We can also look at &lt;code&gt;fits&lt;/code&gt; to see that there is now a &lt;code&gt;fit&lt;/code&gt; list column containing each of the non-linear fits for each combination of our grouping variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at a single fit
summary(fits$fit[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## lnc  -1.3462     0.4656  -2.891   0.0202 *  
## E     0.9877     0.4521   2.185   0.0604 .  
## Eh    4.3326     1.4878   2.912   0.0195 *  
## Th  312.1887     3.8782  80.499 6.32e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9524 on 8 degrees of freedom
## 
## Number of iterations to convergence: 18 
## Achieved convergence tolerance: 1.49e-08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at output object
select(fits, curve_id, data, fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 x 3
##    curve_id data              fit      
##       &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;   
##  1     1.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  2     2.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  3     3.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  4     4.00 &amp;lt;tibble [9 × 3]&amp;gt;  &amp;lt;S3: nls&amp;gt;
##  5     5.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  6     6.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  7     7.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  8     8.00 &amp;lt;tibble [10 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  9     9.00 &amp;lt;tibble [8 × 3]&amp;gt;  &amp;lt;S3: nls&amp;gt;
## 10    10.0  &amp;lt;tibble [10 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
## # ... with 50 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These fits can be cleaned up using the &lt;strong&gt;broom&lt;/strong&gt; functions and &lt;strong&gt;purrr::map()&lt;/strong&gt; to iterate over the grouping variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get summary info
info &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(glance))

# get params
params &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(tidy))

# get confidence intervals
CI &amp;lt;- fits %&amp;gt;% 
  unnest(fit %&amp;gt;% map(~ confint2(.x) %&amp;gt;%
  data.frame() %&amp;gt;%
  rename(., conf.low = X2.5.., conf.high = X97.5..))) %&amp;gt;%
  group_by(., curve_id) %&amp;gt;%
  mutate(., term = c(&amp;#39;lnc&amp;#39;, &amp;#39;E&amp;#39;, &amp;#39;Eh&amp;#39;, &amp;#39;Th&amp;#39;)) %&amp;gt;%
  ungroup()

# merge parameters and CI estimates
params &amp;lt;- merge(params, CI, by = intersect(names(params), names(CI)))

# get predictions
preds &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at &lt;strong&gt;info&lt;/strong&gt; allows us to see if all the models converged.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(info, curve_id, logLik, AIC, BIC, deviance, df.residual)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 x 6
##    curve_id  logLik   AIC   BIC deviance df.residual
##       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;int&amp;gt;
##  1     1.00 -14.0   38.0  40.4     7.26            8
##  2     2.00 - 1.20  12.4  14.8     0.858           8
##  3     3.00 - 7.39  24.8  27.2     2.41            8
##  4     4.00 - 0.523 11.0  12.0     0.592           5
##  5     5.00 -10.8   31.7  34.1     4.29            8
##  6     6.00 - 8.52  27.0  29.5     2.91            8
##  7     7.00 - 1.29  12.6  15.0     0.871           8
##  8     8.00 -13.4   36.7  38.2     8.48            6
##  9     9.00   1.82   6.36  6.76    0.297           4
## 10    10.0  - 1.27  12.5  14.1     0.755           6
## # ... with 50 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When plotting non-linear fits, I prefer to have a smooth curve, even when there are not many points underlying the fit. This can be achieved by including &lt;code&gt;newdata&lt;/code&gt; in the &lt;strong&gt;augment()&lt;/strong&gt; function and creating a higher resolution set of predictor values.&lt;/p&gt;
&lt;p&gt;However, when predicting for many different fits, it is not certain that each curve has the same range of predictor variables. We can get around this by setting the limits of each prediction by the &lt;strong&gt;min()&lt;/strong&gt; and &lt;strong&gt;max()&lt;/strong&gt; of the predictor variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new data frame of predictions
new_preds &amp;lt;- Chlorella_TRC %&amp;gt;%
  do(., data.frame(K = seq(min(.$K), max(.$K), length.out = 150), stringsAsFactors = FALSE))

# max and min for each curve
max_min &amp;lt;- group_by(Chlorella_TRC, curve_id) %&amp;gt;%
  summarise(., min_K = min(K), max_K = max(K)) %&amp;gt;%
  ungroup()

# create new predictions
preds2 &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment, newdata = new_preds)) %&amp;gt;%
  merge(., max_min, by = &amp;#39;curve_id&amp;#39;) %&amp;gt;%
  group_by(., curve_id) %&amp;gt;%
  filter(., K &amp;gt; unique(min_K) &amp;amp; K &amp;lt; unique(max_K)) %&amp;gt;%
  rename(., ln.rate = .fitted) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These can then be plotted using &lt;strong&gt;ggplot2&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot
ggplot() +
  geom_point(aes(K - 273.15, ln.rate, col = flux), size = 2, Chlorella_TRC) +
  geom_line(aes(K - 273.15, ln.rate, col = flux, group = curve_id), alpha = 0.5, preds2) +
  facet_wrap(~ growth.temp + process, labeller = labeller(.multi_line = FALSE)) +
  scale_colour_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  theme(legend.position = c(0.9, 0.15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-12-22-fitting-many-non-linear-regressions-with-broom-purrr-and-nls-multstart_files/figure-html/plot_many_fits-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The confidence intervals of each parameter for each curve fit can also be easily visualised.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot
ggplot(params, aes(col = flux)) +
  geom_point(aes(curve_id, estimate)) +
  facet_wrap(~ term, scale = &amp;#39;free_x&amp;#39;, ncol = 4) +
  geom_linerange(aes(curve_id, ymin = conf.low, ymax = conf.high)) +
  coord_flip() +
  scale_color_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  theme(legend.position = &amp;#39;top&amp;#39;) +
  xlab(&amp;#39;curve&amp;#39;) +
  ylab(&amp;#39;parameter estimate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-12-22-fitting-many-non-linear-regressions-with-broom-purrr-and-nls-multstart_files/figure-html/confint_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This method of modelling can be used for different data, different non-linear models (and linear models for that matter) and combined with the &lt;strong&gt;tidyverse&lt;/strong&gt; can make very useful visualisations.&lt;/p&gt;
&lt;p&gt;The next stage of these curve fits is to try and better understand the uncertainty of these curve fits and their predictions. One approach to achieve this could be bootstrapping new datasets from the existing data. I hope to demonstrate how this could be done soon in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Padfield, D., Yvon-durocher, G., Buckling, A., Jennings, S. &amp;amp; Yvon-durocher, G. (2016). Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton. Ecology Letters, 19(2), 133-142.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Doing local BLAST searches on sanger sequence data in R</title>
      <link>https://padpadpadpad.github.io/post/using-blast-on-sanger-sequences-in-r/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://padpadpadpad.github.io/post/using-blast-on-sanger-sequences-in-r/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Moving to a microbiology group has meant I now have loads of interesting genomic and sequencing data to play with!&lt;/p&gt;
&lt;p&gt;One common task we have when we start with some new natural communities is to work out what is in there! We plate the samples out, look at their colony morphology, but colonies can be picked and sequenced using sanger sequencing. Once you have your sequencing data back, you can check your sequence against a database to look at the taxonomic identity of the picked colonies.&lt;/p&gt;
&lt;p&gt;I know that these sequences can be queried in &lt;a href=&#34;https://blast.ncbi.nlm.nih.gov/Blast.cgi&#34;&gt;BLAST&lt;/a&gt;, where you enter a sequence and it returns matches from a database based on the percentage match. That is all pretty great but I am not about inputting 96 separate sequences into a website! So on Friday I explored the ways to do BLAST searches in a more automated way and came across &lt;a href=&#34;https://github.com/mhahsler/rBLAST&#34;&gt;rBLAST&lt;/a&gt;. &lt;code&gt;rBLAST&lt;/code&gt; is an R package that allows for local blast searches to be done in R.&lt;/p&gt;
&lt;p&gt;This blog post is the culmination of about 5 hours coding that explains how you can go from sequences in a folder, to a dataframe of species names to each sequence. To do this I use &lt;code&gt;rBLAST&lt;/code&gt; to run the local BLAST search, assign taxonomy using the &lt;code&gt;taxise&lt;/code&gt; package, as well as some tidyverse tools from &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;purrr&lt;/code&gt; to make everything work smoothly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-everything-installed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Get everything installed&lt;/h2&gt;
&lt;p&gt;Firstly everything that is used needs to be installed: - BLAST+ needs to be installed externally from R. Advice and instructions for installation can be found &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK279671/&#34;&gt;here&lt;/a&gt; - Install any packages available on CRAN using &lt;code&gt;install.packages(c(&amp;quot;dplyr&amp;quot;, &amp;quot;purrr&amp;quot;, &amp;quot;tidyr&amp;quot;)&lt;/code&gt; - Bioconductor provides tools for the analysis of high throughput genomic data. Some of the packages on their are not available on CRAN but can be installed using instructions on their &lt;a href=&#34;https://www.bioconductor.org/install/&#34;&gt;website&lt;/a&gt;. Specifically we need to download &lt;code&gt;Biostrings&lt;/code&gt; for &lt;code&gt;rBlast&lt;/code&gt; to work. - GitHub packages can be installed using &lt;code&gt;devtools::install_github()&lt;/code&gt;. Install &lt;code&gt;rBLAST&lt;/code&gt; using &lt;code&gt;devtools::install_github(mhahsler/rBLAST)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Hopefully all the necessary packages to run local BLAST searches in R are now installed. Now we can load in some data!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-and-loading-in-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Set up and loading in data&lt;/h2&gt;
&lt;p&gt;The output from sanger sequencing (although I am not well versed in it) appears to be one file per sample and all the output (including consensus scores) is present in .ab1 files. However I also received .seq files that acted as .txt files that stored the output of each sequence in. So in &lt;code&gt;file_1.seq&lt;/code&gt; the output would be “CATGCAGTAGCT…” and so on.&lt;/p&gt;
&lt;p&gt;I am going to use these .seq files to illustrate how to use rBLAST, but this script should be easily adaptable to file types as long as you can access each sequence!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages into R
library(rBLAST)
library(taxize)
library(dplyr)
library(tidyr)
library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we want to list all the files in the data folder (because you keep all your raw data in separate folders and don’t edit them under any circumstances… &lt;strong&gt;RIGHT?!?&lt;/strong&gt;). As an aside, learn about good workflows in R &lt;a href=&#34;https://github.com/jdblischak/workflowr&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://swcarpentry.github.io/r-novice-gapminder/02-project-intro/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# list files in the data folder
seq_files &amp;lt;- list.files(&amp;#39;raw_data&amp;#39;, pattern = &amp;#39;.seq&amp;#39;)

# store file path
seq_file_path &amp;lt;- &amp;#39;where_your_files_are_saved&amp;#39;

# check they been found/we are in the right place
seq_files

# open a single file
read.table(file.path(seq_file_path,seq_file[1]), blank.lines.skip = TRUE, as.is = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;file_1.seq&amp;quot; &amp;quot;file_2.seq&amp;quot; &amp;quot;file_3.seq&amp;quot; &amp;quot;file_4.seq&amp;quot; &amp;quot;file_5.seq&amp;quot;
## [6] &amp;quot;file_6.seq&amp;quot; &amp;quot;file_7.seq&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                                 V1
## 1 NNNNNNNNNNNGNNNNCTGGNCNNAANCGCGCGTAGGTGGNTCGTTAAGTTGGANGTGANTCCCCGGGCTCNNCCTGGGA
## 2 ACTGCATTCAAAACTGACGAGCTAGAGTATGGTAGAGGGTGGTGGAATTTCCTGTGTAGCGGTGAAATGCGTAGATATAG
## 3 GAAGGAACACCAGTGGCGAAGGNGACCACCTGGACTGATACTGACACTGAGGTGCGAAAGCGTGGGGAGCAAACAGGANT
## 4                                                                AGNNNNNNNNGTAGTNN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So our sequence is there, all pretty and ready for alignment. However, to do local BLAST searches, we need to download a database to query our own sequences against. So we will download a 16S Microbial database from the ncbi website. This step only has to be done once and can be done within R, you just need to make a new folder (so the database files are in a separate folder) to extract the zipped files into.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download file
download.file(url = &amp;quot;ftp://ftp.ncbi.nlm.nih.gov/blast/db/16SMicrobial.tar.gz&amp;quot;, destfile = &amp;quot;16SMicrobial.tar.gz&amp;quot;, mode = &amp;#39;wb&amp;#39;)

# extract files from zipped file
untar(&amp;quot;16SMicrobial.tar.gz&amp;quot;, exdir = &amp;quot;blast_16SMicrobial_db&amp;quot;)

# load database into R
bl &amp;lt;- blast(db = &amp;quot;blast_16SMicrobial_db/16SMicrobial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to do local BLAST searches on each file in our sequence folder, &lt;code&gt;seq_file_path&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-multiple-local-blast-searches&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Run multiple local BLAST searches&lt;/h2&gt;
&lt;p&gt;It is common practice in R to do the same function over and over on multiple groups of a column (or elements in a vector) in R. For in my work I often take multiple .csv or .xlsx files, run them through a function where I grab only the information I want, and then bind them into a single dataframe. I used to use &lt;code&gt;plyr::ldply()&lt;/code&gt; for this purpose but have recently moved across to &lt;code&gt;purrr::map_df()&lt;/code&gt; as it is more compatible with &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When going about this process, I first think what I want my output to be and then get the desired output for just a single file/element. Finally, if I can, I wrap the steps up in a function so that it can be rolled out again and again! There are loads of resources on &lt;a href=&#34;http://adv-r.had.co.nz/Functions.html&#34;&gt;writing functions&lt;/a&gt; and &lt;a href=&#34;https://stackoverflow.com/questions/4442518/general-suggestions-for-debugging-in-r&#34;&gt;debugging R code&lt;/a&gt; that should put you in the right direction for your own problems better than I can here.&lt;/p&gt;
&lt;p&gt;In this example, we want a function that takes a single &lt;code&gt;.seq&lt;/code&gt; file and returns a &lt;code&gt;data.frame&lt;/code&gt; containing the BLAST search hit results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function for a single local BLAST search for a single file
rBLAST_all &amp;lt;- function(seq_file, database, keep = 0.95){
  
  # read in and create single sequence line
  test &amp;lt;- read.table(seq_file, blank.lines.skip = TRUE, as.is = TRUE)
  seq1 &amp;lt;- paste(test$V1, collapse = &amp;#39;&amp;#39;)
  
  # make a string set necessary for rBLAST and name file
  seq2 &amp;lt;- Biostrings::BStringSet(seq1)
  names(seq2) &amp;lt;- basename(seq_file)
  
  # use rBLAST and add sequence to subsequent dataframe
  seq_pred &amp;lt;- predict(database, seq2) %&amp;gt;%
    dplyr::filter(., Perc.Ident &amp;gt; keep*100) %&amp;gt;%
    dplyr::mutate_at(., c(&amp;#39;QueryID&amp;#39;, &amp;#39;SubjectID&amp;#39;), as.character) %&amp;gt;%
    dplyr::mutate(seq = seq1)
  
  # return dataframe  
  return(seq_pred)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes a single file, e.g. &lt;code&gt;file_1.seq&lt;/code&gt;, reads it in and gets the sequence. It then makes the dataset into a &lt;code&gt;DNAStringSet&lt;/code&gt;, a special object type in R that rBLAST needs the sequence to be in. I then also assigned the name of the file to the sequence so each row in the final data frame has an identifier.&lt;/p&gt;
&lt;p&gt;I then call &lt;code&gt;rBLAST&lt;/code&gt; and do local BLAST alignment on the sequence using its &lt;code&gt;predict()&lt;/code&gt; function. I have added a parameter, &lt;code&gt;keep&lt;/code&gt;, that controls the % similarity of the hits to retain in the dataframe. Anything under that proportion will be discarded.&lt;/p&gt;
&lt;p&gt;The key to adapting this function for your own needs is to get the desired input and output of your start file, Once you’ve read in one of your files and made it a &lt;code&gt;DNAStringSet&lt;/code&gt; running them through BLAST should be ok.&lt;/p&gt;
&lt;p&gt;We can run this function on a single file and see if the output is what we want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run on the first file
test &amp;lt;- rBLAST_all(file.path(seq_file_path, seq_files[1]), database = bl)

# look at names of columnes
names(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;QueryID&amp;quot;          &amp;quot;SubjectID&amp;quot;        &amp;quot;Perc.Ident&amp;quot;      
##  [4] &amp;quot;Alignment.Length&amp;quot; &amp;quot;Mismatches&amp;quot;       &amp;quot;Gap.Openings&amp;quot;    
##  [7] &amp;quot;Q.start&amp;quot;          &amp;quot;Q.end&amp;quot;            &amp;quot;S.start&amp;quot;         
## [10] &amp;quot;S.end&amp;quot;            &amp;quot;E&amp;quot;                &amp;quot;Bits&amp;quot;            
## [13] &amp;quot;seq&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at first 6 rows of data, seq column removed (LONG!)
head(select(test, -seq))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      QueryID   SubjectID Perc.Ident Alignment.Length Mismatches
## 1 file_1.seq NR_029042.2     95.890              219          8
## 2 file_1.seq NR_126220.1     95.434              219          9
## 3 file_1.seq NR_117177.1     95.434              219          9
## 4 file_1.seq NR_116299.1     95.434              219          9
## 5 file_1.seq NR_028906.1     95.434              219          9
## 6 file_1.seq NR_024950.1     95.434              219          9
##   Gap.Openings Q.start Q.end S.start S.end         E Bits
## 1            1      25   242     560   778 7.09e-102  368
## 2            1      25   242     546   764 3.30e-100  363
## 3            1      25   242     558   776 3.30e-100  363
## 4            1      25   242     541   759 3.30e-100  363
## 5            1      25   242     579   797 3.30e-100  363
## 6            1      25   242     556   774 3.30e-100  363&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Yay!&lt;/strong&gt; A dataframe of alignments much like you would see in BLAST. Pretty amazing. We can then run that function on all of our &lt;code&gt;.seq&lt;/code&gt; files using &lt;code&gt;purrr::map_df()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# update our files to have the full names
seq_files = paste(seq_file_path, seq_files, sep = &amp;#39;/&amp;#39;)

# run rBlast all
seq_IDs &amp;lt;- purrr::map_df(seq_files, rBLAST_all, database = bl, keep = 0.97) %&amp;gt;%
  # create column for nrow
  mutate(., num = 1:n())

# check number of rows
nrow(seq_IDs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a dataframe of 54 hits across the 7 samples. The local BLAST alignments only return a &lt;code&gt;SubjectID&lt;/code&gt; by default, which is the GenBankID number, but we want the species name of each sequence!&lt;/p&gt;
&lt;p&gt;I am going to demonstrate how this can be done using the R package &lt;code&gt;taxize&lt;/code&gt;, that uses the NCBI API to get the taxonomy information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-taxize-to-get-taxonomy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Using taxize to get taxonomy&lt;/h2&gt;
&lt;p&gt;The R package &lt;code&gt;taxize&lt;/code&gt; interacts with a suite of web APIs for taxonomic jobs, such as getting database specific taxonomic identifiers, verifying species names and (&lt;strong&gt;importantly&lt;/strong&gt;), getting species names from database IDs. A great tutorial and overview of &lt;code&gt;taxise&lt;/code&gt; is on the &lt;a href=&#34;https://ropensci.org/tutorials/taxize_tutorial.html&#34;&gt;rOpenSci website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As &lt;code&gt;SubjectID&lt;/code&gt; in our dataframe is a GenBankID, we need to get the NCBI taxonomy UID for each of these and then get the taxonomy. &lt;code&gt;taxize&lt;/code&gt; has functions to do this. To go from GenBankID -&amp;gt; UID we shall use &lt;code&gt;taxize::genbank2uid()&lt;/code&gt;. To go from UID -&amp;gt; taxonomy we shall use &lt;code&gt;taxize::ncbi_get_taxon_summary&lt;/code&gt;. To do the same function on row of our dataframe, I nested some of my data in a list within my existing dataframe. This gives a list column that I can use &lt;code&gt;purrr::map()&lt;/code&gt; to add another list column to my dataframe, before unnesting them both again! If this sounds complex, you are not alone (and this is the first time I did it this way too), but I did find a couple of great examples on list columns, purrr and dplyr &lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://ijlyttle.github.io/isugg_purrr/presentation.html#(1)&#34;&gt;here&lt;/a&gt;. Here, this method helps ensure a single API call is used for each row of the dataframe and helps us control for possible errors, using &lt;code&gt;purrr::possibly()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# reduce number of columns so we drop all of the gumph
seq_IDs &amp;lt;- select(seq_IDs, num, QueryID, SubjectID, Perc.Ident)

# work out the uid from the genbank id - given as SubjectID
# approach - nest each row in a list and call the API separately - return NA if it does not work
seq_IDs_nest &amp;lt;- seq_IDs %&amp;gt;%
  nest(., SubjectID) %&amp;gt;%
  mutate(., uid = map(data, possibly(genbank2uid, otherwise = NA, quiet = TRUE)))

# unnest these columns to get the dataframes out. There shall be warnings but they are ok as the correct uids are preserved
seq_IDs &amp;lt;- seq_IDs_nest %&amp;gt;%
  unnest(data, uid)

head(seq_IDs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   num    QueryID Perc.Ident     uid   SubjectID
## 1   1 file_3.seq     97.403 1775474 NR_134828.1
## 2   2 file_4.seq     97.854 1649133 NR_145922.1
## 3   3 file_4.seq     97.854  123899 NR_118635.1
## 4   4 file_4.seq     97.854 1287737 NR_117708.1
## 5   5 file_4.seq     97.854 1287736 NR_117707.1
## 6   6 file_4.seq     97.854 1287735 NR_117706.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the NCBI taxon summary ID, we can go ahead and get the taxonomy of each row, in much the same way as we got the NCBI taxon summary ID from the GenBank ID.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now nest uids of each row and run the get_taxon_summary API call on each list element
seq_IDs_nest &amp;lt;- seq_IDs %&amp;gt;%
  nest(., uid) %&amp;gt;%
  mutate(., tax_info = map(data, possibly(ncbi_get_taxon_summary, otherwise = NA, quiet = TRUE)))

# unnest this dataframe for final dataframe of what everything is
seq_IDs &amp;lt;- seq_IDs_nest %&amp;gt;%
  unnest(tax_info) %&amp;gt;%
  select(., -data)

# lets have a look
head(seq_IDs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   num    QueryID Perc.Ident   SubjectID     uid                       name
## 1   1 file_3.seq     97.403 NR_134828.1 1775474    Variovorax guangxiensis
## 2   2 file_4.seq     97.854 NR_145922.1 1649133      Bordetella tumulicola
## 3   3 file_4.seq     97.854 NR_118635.1  123899        Bordetella trematum
## 4   4 file_4.seq     97.854 NR_117708.1 1287737      Achromobacter anxifer
## 5   5 file_4.seq     97.854 NR_117707.1 1287736 Achromobacter aegrifaciens
## 6   6 file_4.seq     97.854 NR_117706.1 1287735     Achromobacter insuavis
##      rank
## 1 species
## 2 species
## 3 species
## 4 species
## 5 species
## 6 species&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there you have it. We have a dataframe that now includes the species name of each of the returned hits for each file. Pretty neat if you ask me, and better than clicking over and over in BLAST on their website! This script is now ready to go again and again. Not bad for around 5 hours work! From this we can examine consensus within samples and look for variation across samples. The opportunities are endless.&lt;/p&gt;
&lt;p&gt;Thanks for reading. A script containing only the code chunks used here can be found &lt;a href=&#34;https://gist.github.com/padpadpadpad/d2fe494b8392d0c84b40fe158caca834&#34;&gt;here&lt;/a&gt; and the files can be found in the &lt;code&gt;data/20171008_sanger&lt;/code&gt; folder of my &lt;a href=&#34;https://github.com/padpadpadpad/blogdown_source&#34;&gt;GitHub blogdown repository&lt;/a&gt; if you want to run through my example completely. You will just have to change the directories where the files are (or preferably use R projects)!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introducing nlsLoop</title>
      <link>https://padpadpadpad.github.io/post/introducing-nlsloop/</link>
      <pubDate>Thu, 08 Sep 2016 21:13:14 -0500</pubDate>
      
      <guid>https://padpadpadpad.github.io/post/introducing-nlsloop/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;So you have data you want to fit to a non-linear model…&lt;/p&gt;
&lt;p&gt;This blog post assumes you have a non-linear model formulation you want to fit to non-linear data and are aware of the excellent &lt;code&gt;nlme&lt;/code&gt; package by Pinheiro and Bates and their &lt;a href=&#34;Mixed-Effects%20Models%20in%20S%20and%20S-PLUS%20(Statistics%20and%20Computing)&#34;&gt;book&lt;/a&gt; that nicely explains how to use it. This would be my first port of call if you want to understand the simplicities and complexities of fitting non-linear models and mixed effect models in general.&lt;/p&gt;
&lt;p&gt;This blog post is less about when to use non-linear regression or non-linear mixed effects models but more about a method of ensuring the &lt;strong&gt;best fit each curve&lt;/strong&gt; to its data.&lt;/p&gt;
&lt;p&gt;When fitting non-linear models to data, it is common to want to fit the same model to multiple levels of a factor. In this situation &lt;code&gt;nlme::nlsList()&lt;/code&gt; is often used to loop through levels of factor such as &lt;code&gt;nlsList(Y ~ function(X)|factor)&lt;/code&gt;. However, &lt;code&gt;nlsList()&lt;/code&gt; only allows a single set of starting parameters.&lt;/p&gt;
&lt;p&gt;If you have a dataset where every level of the factor fits the same model, but spans several magnitudes of parameter values, you will be familiar with the error:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Error in nlsList(Y ~ function(X)|factor): singular gradient&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;that is often caused by starting parameters being too far away from the actual values in parameter space. This often means &lt;code&gt;nlsList()&lt;/code&gt; is inadequate for fitting multiple non-linear models.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nlsLoop&lt;/code&gt; is a package hosted on &lt;a href=&#34;https://github.com/padpadpadpad/TeamPhytoplankton&#34;&gt;GitHub&lt;/a&gt; that allows multiple starting values to be used to ensure the best fit to each separate curve. &lt;code&gt;nlsLoop::nlsLoop()&lt;/code&gt; fits the same model over a level of a factor and allows multiple starting values and picks the best model using AIC score.&lt;/p&gt;
&lt;div id=&#34;key-parameters-of-nlsloop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Key parameters of nlsLoop()&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;model: the non-linear model specification&lt;/li&gt;
&lt;li&gt;data: the raw data frame&lt;/li&gt;
&lt;li&gt;tries: the number of combinations of starting values you want the model to try&lt;/li&gt;
&lt;li&gt;id_col: the column that has a unique identifier for each specific curve you want to model&lt;/li&gt;
&lt;li&gt;param_bds: a vector specifying the boundaries of your starting parameter estimates. It is important to &lt;strong&gt;LOOK&lt;/strong&gt; at your data and previous literature at this point to try and work what reasonable ranges of parameters are!
&lt;ul&gt;
&lt;li&gt;These are specified as c(lower bound parameter 1, upper bound parameter 1, lower bound parameter 2, upper bound parameter 2, lower bound parameter &lt;em&gt;n&lt;/em&gt;, upper bound parameter &lt;em&gt;n&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;r&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; : whether or not you want to calculate an &lt;em&gt;r&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; value for each of your fits. This parameter comes with a &lt;a href=&#34;http://stackoverflow.com/questions/14530770/calculating-r2-for-a-nonlinear-model&#34;&gt;warning&lt;/a&gt; that &lt;em&gt;r&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; do not mean the same things for non-linear models as they do for linear models&lt;sup&gt;[1]&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-nlsloop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using nlsLoop&lt;/h2&gt;
&lt;div id=&#34;load-in-packages-and-install-nlsloop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Load in packages and install nlsLoop&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlsLoop) # devtools::install_github(&amp;#39;padpadpadpad/nlsLoop&amp;#39;)
library(dplyr)
library(magrittr)
library(tidyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;load-in-data-frame-and-initial-plot&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Load in data frame and initial plot&lt;/h4&gt;
&lt;p&gt;The data we shall use for this tutorial are a bunch of thermal performance curves showing the relationship between metabolic rates and temperature in the aquatic alga, &lt;em&gt;Chlorella vulgaris&lt;/em&gt;&lt;sup&gt;[2]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;So enough talking, lets load in the data and have an initial look at the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;Chlorella_TRC&amp;quot;)

head(Chlorella_TRC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   curve_id growth.temp     process        flux temp      K    ln.rate
## 1        1          20 acclimation respiration   16 289.15 -2.0625783
## 2        1          20 acclimation respiration   19 292.15 -1.3243794
## 3        1          20 acclimation respiration   22 295.15 -0.9541681
## 4        1          20 acclimation respiration   25 298.15 -0.7944367
## 5        1          20 acclimation respiration   28 301.15 -0.1820364
## 6        1          20 acclimation respiration   31 304.15  0.1742401&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the Sharpe-Schoolfield equation
schoolfield_high &amp;lt;- function(lnc, E, Eh, Th, temp, Tc) {
    Tc &amp;lt;- 273.15 + Tc
    k &amp;lt;- 8.62e-05
    boltzmann.term &amp;lt;- lnc + log(exp(E/k * (1/Tc - 1/temp)))
    inactivation.term &amp;lt;- log(1/(1 + exp(Eh/k * (1/Th - 1/temp))))
    return(boltzmann.term + inactivation.term)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, and read about in the &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/ele.12545/full&#34;&gt;paper&lt;/a&gt; (&lt;strong&gt;YES, THIS IS A PLUG&lt;/strong&gt;), we have rates of metabolism for both photosynthesis and respiration across 5 different growth temperatures after both 10 (acclimation) and 100 (adaptation) generations of growth, so we can have a go at an initial plot accordingly…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Chlorella_TRC) +
  geom_point(aes(temp, ln.rate, col = growth.temp), shape = 21, size = 2, alpha = 0.5) +
  facet_wrap(~flux + process, labeller = labeller(.multi_line = F)) +
  scale_colour_gradient(low = &amp;#39;blue&amp;#39;, high = &amp;#39;red&amp;#39;, &amp;#39;Growth Temperature&amp;#39;) +
  theme_bw(base_size = 16, base_family = &amp;#39;Helvetica&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-09-07-introducing-nlsloop_files/figure-html/first%20plot-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data follows a classic unimodal thermal performance curve and can be modelled using a modified version of the Sharpe-Schoolfield model&lt;sup&gt;[3]&lt;/sup&gt;. A function characterising the model is provided in the package as &lt;code&gt;schoolfield.high()&lt;/code&gt;. This is more about the fitting of non-linear curves than the model so that’s as in depth as I will go here!&lt;/p&gt;
&lt;p&gt;The data has 60 separate curves and &lt;code&gt;id_col = curve_id&lt;/code&gt;. So lets fit the model!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-nlsloop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. Run nlsLoop&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;nlsLoop::nlsLoop()&lt;/code&gt; uses &lt;code&gt;minpack.lm::nlsLM()&lt;/code&gt; to fit each individual curve allows addiitional arguments such as &lt;code&gt;lower&lt;/code&gt; and &lt;code&gt;na.action&lt;/code&gt; accordingly. The best fit for each factor level is determined using AIC scores and if the best fit does not change for 100 “tries” then it will move onto the next level of the factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits &amp;lt;- nlsLoop(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                     data = Chlorella_TRC,
                     tries = 500,
                     id_col = &amp;#39;curve_id&amp;#39;,
                     param_bds = c(-10, 10, 0.1, 2, 0.5, 5, 285, 330),
                     r2 = &amp;#39;Y&amp;#39;,
                     supp_errors = &amp;#39;Y&amp;#39;,
                     AICc = &amp;#39;Y&amp;#39;,
                     na.action = na.omit,
                     lower = c(ln.c=-10, Ea=0, Eh=0, Th=0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;WARNING: This may take quite a while depending on how good your parameter boundaries are!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The result of this returns an &lt;code&gt;nlsLoop&lt;/code&gt; object that is made up of &lt;code&gt;params&lt;/code&gt; which has all of our parameter values for each individual fit and &lt;code&gt;predictions&lt;/code&gt; which is a high resolution predicted dataframe to allow for a smooth curve on a plot (which we will come to later).&lt;/p&gt;
&lt;p&gt;These can be accessed simply using &lt;code&gt;fits$params&lt;/code&gt; or &lt;code&gt;fits$predictions&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fits$params)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   curve_id       lnc         E       Eh       Th      AIC  quasi_r2
## 1        1 -1.346211 0.9877307 4.332645 312.1887 48.01896 0.4608054
## 2        2 -1.349431 1.0653450 4.211374 312.6591 22.39398 0.8978426
## 3        3 -1.815315 1.1155334 4.140395 310.9545 34.77114 0.7804032
## 4        4 -1.612615 1.0982576 3.025816 310.6412 31.04688 0.8709134
## 5        5 -1.767711 1.1244277 9.010641 317.0688 41.69970 0.7602547
## 6        6 -1.717258 1.1727047 4.077252 311.4596 37.03555 0.7289198&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having all of these in a single &lt;code&gt;nlsLoop&lt;/code&gt; object allows a simple plotting method to assess how good our fits to the data are. One of the best ways to assess non-linear model is by &lt;strong&gt;ACTUALLY LOOKING&lt;/strong&gt; at how well every model fits its respective raw data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4. Plotting&lt;/h4&gt;
&lt;p&gt;Firstly lets have a look at a single level of &lt;code&gt;curve_id&lt;/code&gt;, a function called &lt;code&gt;plot_id_nlsLoop()&lt;/code&gt; allows this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_id_nlsLoop(data = Chlorella_TRC, param_data = fits, id = &amp;#39;1&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-09-07-introducing-nlsloop_files/figure-html/first%20fit%20plot-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not a bad fit… Further to this, &lt;code&gt;plot_all_nlsLoop()&lt;/code&gt; will produce a pdf with each plot on a new sheet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_all_nlsLoop(&amp;#39;path/of/where/you/want/to/save/me.pdf&amp;#39;, data = Chlorella_TRC, param_data = fits)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analysis&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5. Exploratory analysis&lt;/h4&gt;
&lt;p&gt;We can now edit some of the dataframes to look at the curves split by the treatments within the dataset and explore how the parameter values change across the treatments.&lt;/p&gt;
&lt;p&gt;For example we can plot all the curves split by respiration, photosynthesis and acclimatory or adaptive timescales.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select treatment columns from original dataset
treatments &amp;lt;- select(Chlorella_TRC, c(curve_id, process, growth.temp, flux))
# keep unique combinations of treatments
treatments &amp;lt;- unique(treatments[,colnames(treatments)])

# bind treatments to predictions and parameter dataframe
fits$params &amp;lt;- merge(fits$params, treatments, by = &amp;#39;curve_id&amp;#39;)
fits$predictions &amp;lt;- merge(fits$predictions, treatments, by = &amp;#39;curve_id&amp;#39;)

# plot every curve
ggplot() +
  geom_point(aes(K, ln.rate, col = growth.temp), shape = 21, size = 2, alpha = 0.25, Chlorella_TRC) +
  geom_line(aes(K, ln.rate, col = growth.temp, group = curve_id), linetype = 2, alpha = 0.5, fits$predictions) +
  facet_wrap(~flux + process, labeller = labeller(.multi_line = F)) +
  scale_colour_gradient(low = &amp;#39;blue&amp;#39;, high = &amp;#39;red&amp;#39;, &amp;#39;Growth Temperature&amp;#39;) +
  theme_bw(base_size = 16, base_family = &amp;#39;Helvetica&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-09-07-introducing-nlsloop_files/figure-html/data%20wrangling-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also start having a quick look at any differences between parameters…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gather(fits$params, &amp;#39;parameter&amp;#39;, &amp;#39;value&amp;#39;, c(lnc, E, Eh, Th)) %&amp;gt;%
  ggplot(.) +
  geom_point(aes(growth.temp, value, col = flux, shape = process), fill = &amp;#39;white&amp;#39;, size = 2, position = position_dodge(width = 0.25)) +
  facet_wrap(~ parameter, scales = &amp;#39;free_y&amp;#39;) +
  scale_color_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  scale_shape_manual(values = c(21, 24)) +
  theme_bw(base_size = 16, base_family = &amp;#39;Helvetica&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-09-07-introducing-nlsloop_files/figure-html/parameter%20plots-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From here we can see that the activation energy (&lt;em&gt;E&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt;) in the model does not change systematically with growth temperature or whether it has acclimated or adapted to the growth temperature. However a body of work has shown that photosynthesis should be less sensitive to temperature change (a lower &lt;em&gt;E&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt;) than that of respiration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(fits$params, aes(flux, E)) +
  geom_boxplot(aes(fill = flux, col = flux), outlier.shape = NA, width = 0.5) +
  stat_summary(geom = &amp;#39;crossbar&amp;#39;, fatten = 0, color = &amp;#39;white&amp;#39;, width = 0.5, fun.data = function(x){ return(c(y=median(x), ymin=median(x), ymax=median(x)))}) +
  geom_jitter(aes(flux, E, col = flux), shape = 21, fill =&amp;#39;white&amp;#39;, width = 0.25, height = 0) +
  theme_bw(base_size = 16, base_family = &amp;#39;Helvetica&amp;#39;) +
  scale_color_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  scale_fill_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  ylab(&amp;#39;Activation enrrgy (eV)&amp;#39;) +
  xlab(&amp;#39;Flux&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://padpadpadpad.github.io/post/2017-09-07-introducing-nlsloop_files/figure-html/Ea%20plots-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And as we can see it looks like we have a difference in the activation energies, with photosynthesis having a lower activation energy than respiration.&lt;/p&gt;
&lt;p&gt;So from there we can start exploring our data and formally analysing our parameters! &lt;code&gt;nlsLoop&lt;/code&gt; allows us to have confidence in our fitting of all the curves and gives us an easy way of plotting and manipulating the data afterwards!&lt;/p&gt;
&lt;p&gt;Happy model fitting. Any comments much appreciated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;p&gt;[1] Spiess, A.N. &amp;amp; Neumeyer, N. (2010). An evaluation of R&lt;sup&gt;2&lt;/sup&gt; as an inadequate measure for nonlinear models in pharmacological and biochemical research: a Monte Carlo approach. BMC Pharmacology, 10, 6.&lt;/p&gt;
&lt;p&gt;[2] Padfield, D., Yvon-durocher, G., Buckling, A., Jennings, S. &amp;amp; Yvon-durocher, G. (2015). Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton. Ecology Letters, 19(2), 133-142.&lt;/p&gt;
&lt;p&gt;[3] Schoolfield, R.M., Sharpe, P.J. &amp;amp; Magnuson, C.E. (1981). Non-linear regression of biological temperature-dependent rate models based on absolute reaction-rate theory. J. Theoretical Biology, 88, 719–31.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
