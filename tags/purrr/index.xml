<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Purrr on Daniel Padfield</title>
    <link>/tags/purrr/</link>
    <description>Recent content in Purrr on Daniel Padfield</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Daniel Padfield</copyright>
    <lastBuildDate>Sun, 21 Jan 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/purrr/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Bootstrap non-linear regression with purrr and modelr</title>
      <link>/post/bootstrapping-non-linear-regressions-with-purrr/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bootstrapping-non-linear-regressions-with-purrr/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For my first academic publication, a reviewer asked for the &lt;span class=&#34;math inline&#34;&gt;\(r^{2}\)&lt;/span&gt; values of the thermal performance curves I fitted using non-linear regression. I bowed to the request as is often the case with reviewer comments, but would now resist as the &lt;span class=&#34;math inline&#34;&gt;\(r^{2}\)&lt;/span&gt; is not necessarily an effective goodness of fit measure for non-linear regression (see this &lt;a href=&#34;https://stackoverflow.com/questions/14530770/calculating-r2-for-a-nonlinear-model&#34;&gt;SO answer&lt;/a&gt;). It does raise the question of how to determine how well a biologically meaningful model fits the data it is fitted to. I generally just plot every curve to its data, but it tells me nothing of the uncertainty around the curve.&lt;/p&gt;
&lt;p&gt;Step forward the bootstrap! Bootstrapping involes simulating “new” datasets produced from the existing data by sampling with replacement. The same model is then fitted separately on each individual bootstrapped dataset. Doing this over and over allows us to visualise uncertainty of predictions and produce confidence intervals of estimated parameters. This blog post was inspired by posts by &lt;a href=&#34;https://rstudio-pubs-static.s3.amazonaws.com/19698_a4c472606e3c43e4b94720506e49bb7b.html&#34;&gt;Andrew MacDonald&lt;/a&gt; and &lt;a href=&#34;https://github.com/tidyverse/dplyr/issues/269&#34;&gt;Hadley Wickham&lt;/a&gt;, as well as a &lt;a href=&#34;https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html&#34;&gt;broom vignette&lt;/a&gt; which use this approach. I have taken their approaches and again applied them to thermal performance curves. The &lt;strong&gt;broom&lt;/strong&gt; approach in these blog posts has since been replaced by &lt;strong&gt;modelr::bootstrap()&lt;/strong&gt;, another package of the &lt;strong&gt;tidyverse&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrapping-predictions-for-a-single-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrapping predictions for a single curve&lt;/h2&gt;
&lt;p&gt;I will demonstrate this approach by using the thermal performance curves for phytoplankton metabolism that I used in a previous &lt;a href=&#34;https://padpadpadpad.github.io/post/fitting-non-linear-regressions-with-broom-purrr-and-nls.multstart/&#34;&gt;post&lt;/a&gt;. The Sharpe-Schoolfield equation and meaning of the parameters can be found in more detail in the previous post.&lt;/p&gt;
&lt;p&gt;Firstly lets load the packages used in the script and write the Sharpe-Schoolfield model as a function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(nls.multstart) # devtools::install_github(&amp;#39;padpadpadpad/nls.multstart&amp;#39;)
library(patchwork) # devtools::install_github(&amp;#39;thomasp85/patchwork&amp;#39;)
library(ggplot2)
library(broom)
library(purrr)
library(dplyr)
library(tidyr)
library(nlstools)
library(modelr)

# write function for sharpe schoolfield model
schoolfield_high &amp;lt;- function(lnc, E, Eh, Th, temp, Tc) {
  Tc &amp;lt;- 273.15 + Tc
  k &amp;lt;- 8.62e-5
  boltzmann.term &amp;lt;- lnc + log(exp(E/k*(1/Tc - 1/temp)))
  inactivation.term &amp;lt;- log(1/(1 + exp(Eh/k*(1/Th - 1/temp))))
  return(boltzmann.term + inactivation.term)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then load in the data and have a look at the its structure using &lt;strong&gt;glimpse()&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load in data
data(Chlorella_TRC)

# look at data
glimpse(Chlorella_TRC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 649
## Variables: 7
## $ curve_id    &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,...
## $ growth.temp &amp;lt;dbl&amp;gt; 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20...
## $ process     &amp;lt;chr&amp;gt; &amp;quot;acclimation&amp;quot;, &amp;quot;acclimation&amp;quot;, &amp;quot;acclimation&amp;quot;, &amp;quot;accl...
## $ flux        &amp;lt;chr&amp;gt; &amp;quot;respiration&amp;quot;, &amp;quot;respiration&amp;quot;, &amp;quot;respiration&amp;quot;, &amp;quot;resp...
## $ temp        &amp;lt;dbl&amp;gt; 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 16...
## $ K           &amp;lt;dbl&amp;gt; 289.15, 292.15, 295.15, 298.15, 301.15, 304.15, 30...
## $ ln.rate     &amp;lt;dbl&amp;gt; -2.06257833, -1.32437939, -0.95416807, -0.79443675...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 60 curves here, 30 each for photosynthesis and respiration. The treatments are growth temperature (20, 23, 27, 30, 33 ºC) and adaptive process (acclimation or adaptation) that reflects the number of generations cultures were grown at each temperature. Bootstrapping the uncertainty on each individual curve is difficult for thermal performance curves because rates generally rapidly decrease after the optimum temperature, &lt;span class=&#34;math inline&#34;&gt;\(T_{opt}\)&lt;/span&gt;, making data collection difficult.&lt;/p&gt;
&lt;p&gt;This means it is likely some of the bootstraps will not include points after the optimum, making the unimodal model formulation unsuitable. Because of this I will pool the replicates curves within treatments together to demonstrate the bootstrapping approach, giving 20 curves in total. This post therefore ignores the non-independence of data points within replicates (a little naughty!).&lt;/p&gt;
&lt;p&gt;To bootstrap a single curve, we can filter the dataset for a single flux (photosynthesis) at a singe growth temperature (20 ºC) and generations of growth (~ 100) and plot the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# filter for one curve
d_examp &amp;lt;- filter(Chlorella_TRC, growth.temp == 20, flux == &amp;#39;photosynthesis&amp;#39;, process == &amp;#39;adaptation&amp;#39;)

# plot 
ggplot(d_examp, aes(K - 273.15, ln.rate)) +
  geom_point(col = &amp;#39;green4&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/filter_data_and_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use &lt;strong&gt;nls_multstart()&lt;/strong&gt;, that allows for &lt;a href=&#34;https://github.com/padpadpadpad/nls.multstart&#34;&gt;multiple start parameters&lt;/a&gt;, to fit a single model to the data. We can then use &lt;strong&gt;tidy()&lt;/strong&gt; and &lt;strong&gt;augment()&lt;/strong&gt; from &lt;strong&gt;broom&lt;/strong&gt; to get the parameters and predictions of the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run nls_multstart
fit &amp;lt;- nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                     data = d_examp,
                     iter = 500,
                     param_bds = c(-10, 10, 0.1, 2, 0.5, 5, 285, 330),
                     supp_errors = &amp;#39;Y&amp;#39;,
                     AICc = &amp;#39;Y&amp;#39;,
                     na.action = na.omit,
                     lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))

# broom functions to tidy up model
params &amp;lt;- tidy(fit)
preds &amp;lt;- augment(fit)

# plot with predictions
ggplot(d_examp, aes(K - 273.15, ln.rate)) +
  geom_point(col = &amp;#39;green4&amp;#39;) +
  geom_line(aes(K - 273.15, .fitted), preds) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  ggtitle(&amp;#39;Single TPC with fitted model&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/run_nls_mulstart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This fit provides p-values and confidence intervals can be calculated using &lt;strong&gt;nlstools::confint2()&lt;/strong&gt;. However, bootstrapping can provide confidence intervals around predictions and for estimated parameters.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bootstrap()&lt;/strong&gt; function in &lt;strong&gt;modelr&lt;/strong&gt; samples bootstrap replicates (here we do 200), each of which is randomly sampled with replacement. This creates a list column in our &lt;strong&gt;tibble&lt;/strong&gt; called &lt;code&gt;strap&lt;/code&gt; which contains the bootsrapped dataset, and a new column called &lt;code&gt;boot_num&lt;/code&gt; that is the number of that bootstrap (from 1 to 200).&lt;/p&gt;
&lt;p&gt;We can then create a new list column of the fit for each &lt;code&gt;strap&lt;/code&gt; using &lt;strong&gt;purrr::map()&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_boots &amp;lt;- d_examp %&amp;gt;% 
  modelr::bootstrap(n = 200, id = &amp;#39;boot_num&amp;#39;) %&amp;gt;%
  group_by(boot_num) %&amp;gt;%
  mutate(fit = map(strap, ~nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                        data = data.frame(.),
                        iter = 100,
                        param_bds = c(-10, 10, 0.1, 2, 0.5, 5, 285, 330),
                        lower = c(lnc=-10, E=0, Eh=0, Th=0),
                        upper = c(lnc = 5, E = 10, Eh = 30, Th = 350),
                        supp_errors = &amp;#39;Y&amp;#39;)
  ))

fit_boots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 200 x 3
## # Groups: boot_num [200]
##    strap          boot_num fit      
##    &amp;lt;list&amp;gt;         &amp;lt;chr&amp;gt;    &amp;lt;list&amp;gt;   
##  1 &amp;lt;S3: resample&amp;gt; 001      &amp;lt;S3: nls&amp;gt;
##  2 &amp;lt;S3: resample&amp;gt; 002      &amp;lt;S3: nls&amp;gt;
##  3 &amp;lt;S3: resample&amp;gt; 003      &amp;lt;S3: nls&amp;gt;
##  4 &amp;lt;S3: resample&amp;gt; 004      &amp;lt;S3: nls&amp;gt;
##  5 &amp;lt;S3: resample&amp;gt; 005      &amp;lt;S3: nls&amp;gt;
##  6 &amp;lt;S3: resample&amp;gt; 006      &amp;lt;S3: nls&amp;gt;
##  7 &amp;lt;S3: resample&amp;gt; 007      &amp;lt;S3: nls&amp;gt;
##  8 &amp;lt;S3: resample&amp;gt; 008      &amp;lt;S3: nls&amp;gt;
##  9 &amp;lt;S3: resample&amp;gt; 009      &amp;lt;S3: nls&amp;gt;
## 10 &amp;lt;S3: resample&amp;gt; 010      &amp;lt;S3: nls&amp;gt;
## # ... with 190 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each bootstrap replicate is stored in a list column within the &lt;strong&gt;tibble&lt;/strong&gt; . This then allows us to apply the &lt;strong&gt;tidy()&lt;/strong&gt; and &lt;strong&gt;augment()&lt;/strong&gt; functions used earlier, using &lt;strong&gt;unnest()&lt;/strong&gt; to combine the list column into a dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get parameters ####
params_boot &amp;lt;- fit_boots %&amp;gt;%
  unnest(fit %&amp;gt;% map(tidy)) %&amp;gt;%
  ungroup()

# get predictions
preds_boot &amp;lt;- fit_boots %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using these two dataframes, we can plot each set of bootstrapped predictions alongside the fit of the original data, &lt;code&gt;preds&lt;/code&gt;, and plot the distribution of each estimated parameter.&lt;/p&gt;
&lt;p&gt;The relatively new package &lt;a href=&#34;https://github.com/thomasp85/patchwork&#34;&gt;patchwork&lt;/a&gt; by Thomas Lin Pedersen can help add multiple graphs together simply by saying &lt;code&gt;plot_1 + plot_2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot distribution of estimated parameters
p1 &amp;lt;- ggplot(params_boot, aes(estimate)) +
  geom_histogram(col = &amp;#39;black&amp;#39;, fill = &amp;#39;white&amp;#39;) +
  facet_wrap(~ term, scales = &amp;#39;free_x&amp;#39;)

# plot points with predictions
p2 &amp;lt;- ggplot() +
  geom_line(aes(K - 273.15, .fitted, group = boot_num), preds_boot, alpha = .03) +
  geom_line(aes(K - 273.15, .fitted), preds) +
  geom_point(aes(K - 273.15, ln.rate), d_examp, col = &amp;#39;green4&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;)
  
# plot both
p1 + p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/get_confidence_intervals-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can smooth our predictions over smaller increments of our predictor variable by passing a new dataset to &lt;strong&gt;augment()&lt;/strong&gt;. Alongside this, for every value of the predictor we can calculate the 2.5% and 97.5% quantiles which gives confidence bands around the predictions.&lt;/p&gt;
&lt;p&gt;Personally I prefer this approach rather than plotting each bootstrapped replicate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new data frame of predictions
new_preds &amp;lt;- d_examp %&amp;gt;%
  do(., data.frame(K = seq(min(.$K), max(.$K), length.out = 250), stringsAsFactors = FALSE))

# create smooth predictions for best fit
preds &amp;lt;- augment(fit, newdata = new_preds)

# create smoother predictions for bootstrapped replicate
preds &amp;lt;- fit_boots %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment, newdata = new_preds)) %&amp;gt;%
  # group by each value of K and get quantiles
  group_by(., K) %&amp;gt;%
  summarise(lwr_CI = quantile(.fitted, 0.025),
            upr_CI = quantile(.fitted, 0.975)) %&amp;gt;%
  ungroup() %&amp;gt;%
  merge(., preds, by = &amp;#39;K&amp;#39;)

# plot
ggplot() +
  geom_point(aes(K - 273.15, ln.rate), d_examp) +
  geom_line(aes(K - 273.15, .fitted), preds) +
  geom_ribbon(aes(K - 273.15, ymin = lwr_CI, ymax = upr_CI), fill = &amp;#39;green4&amp;#39;, preds, alpha = .2) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  ggtitle(&amp;#39;Single TPC with confidence intervals&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/plot_smooth_preds-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is starting to look pretty nice, and is a great approach to visualising uncertainty of non-linear regressions for many types of data.&lt;/p&gt;
&lt;p&gt;From &lt;code&gt;params_boot&lt;/code&gt; we can calculate confidence intervals of each estimated parameter by taking the desired quantiles of the data. This can be compared the output from &lt;strong&gt;confint2()&lt;/strong&gt; from &lt;strong&gt;nlstools&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate confidence intervals of estimated parameters using confint2()
confint_1 &amp;lt;- confint2(fit) %&amp;gt;%
  data.frame() %&amp;gt;%
  rename(., conf_low = X2.5.., conf_high = X97.5..) %&amp;gt;%
  mutate(method = &amp;#39;nlstools&amp;#39;) %&amp;gt;%
  cbind(., select(params, term, estimate)) %&amp;gt;%
  select(., term, estimate, conf_low, conf_high, method)

# calculate confidence intervals using bootstraps
confint_2 &amp;lt;- group_by(params_boot, term) %&amp;gt;%
  summarise(.,
            conf_low = quantile(estimate, 0.025),
            conf_high = quantile(estimate, 0.975),
            estimate = quantile(estimate, 0.5)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(., method = &amp;#39;boot&amp;#39;)

# bind both methods
confint &amp;lt;- bind_rows(confint_1, confint_2)

# plot each method side by side
ggplot(confint, aes(method, estimate, col = method)) +
  geom_point(size = 3) +
  geom_linerange(aes(ymin = conf_low, ymax = conf_high)) +
  facet_wrap(~ term, scales = &amp;#39;free_y&amp;#39;) +
  theme_bw() +
  theme(legend.position = &amp;#39;none&amp;#39;) +
  ggtitle(&amp;#39;Comparison of confidence interval calculation for estimated parameters&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/confint-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bootstrapping gives similar mean estimates, but gives wider, asymmetric, confidence intervals compared to those calculated using &lt;strong&gt;nlstools::confint2()&lt;/strong&gt;. In this instance this could be because I am only running 200 bootstrap replicates, whereas the number of bootstraps done in published analyses are commonly around 10,000.&lt;/p&gt;
&lt;p&gt;Crucially, bootstrapping allows the calculation of confidence intervals for parameters derived from the model that were not present in the initial fitting process. For example, the optimum temperature of a thermal performance curve, &lt;span class=&#34;math inline&#34;&gt;\(T_{opt}\)&lt;/span&gt; is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_{opt} = \frac{E_{h}T_{h}}{E_{h} + k T_{h} ln(\frac{E_{h}}{E} - 1)}\]&lt;/span&gt; We can calculate &lt;span class=&#34;math inline&#34;&gt;\(T_{opt}\)&lt;/span&gt; for each iteration of the bootstrap and then plot the distribution of derived parameters. This can be done by using &lt;strong&gt;tidyr::spread()&lt;/strong&gt; to have a column for each estimated parameter, and then calculate &lt;code&gt;Topt&lt;/code&gt; for each &lt;code&gt;boot_num&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function for calculating Topt
Topt &amp;lt;- function(E, Th, Eh){
  return((Eh*Th)/(Eh + (8.62e-05 *Th*log((Eh/E) - 1))))
}

Topt_boot &amp;lt;- select(params_boot, boot_num, term, estimate) %&amp;gt;%
  spread(., term, estimate) %&amp;gt;%
  mutate(., Topt = Topt(E, Th, Eh))

# plot distribution of Topt
ggplot(Topt_boot, aes(Topt - 273.15)) +
  geom_histogram(col = &amp;#39;black&amp;#39;, fill = &amp;#39;white&amp;#39;) +
  xlab(&amp;#39;Optimum Temperature (ºC)&amp;#39;) +
  ggtitle(&amp;#39;Distribution of optimum temperatures via bootstrapping&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/Topt-1.png&#34; width=&#34;672&#34; /&gt; Bootstrapping allows us to get uncertainty estimates for parameters outside of the original curve fitting process!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrapping-multiple-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrapping multiple curves&lt;/h2&gt;
&lt;p&gt;Bootstrapping over each curve can be done by combining functions from the tidyverse to the &lt;strong&gt;bootstrap()&lt;/strong&gt; call. To fit a single model to each curve, I use &lt;strong&gt;nest()&lt;/strong&gt;, &lt;strong&gt;mutate()&lt;/strong&gt; and &lt;strong&gt;map()&lt;/strong&gt; as shown &lt;a href=&#34;https://padpadpadpad.github.io/post/fitting-non-linear-regressions-with-broom-purrr-and-nls.multstart&#34;&gt;previously&lt;/a&gt;. I searched for a way of using the same workflow for bootstrapping, and finally came across the &lt;a href=&#34;https://github.com/tidyverse/broom/issues/25&#34;&gt;answer&lt;/a&gt;. Each element of &lt;code&gt;strap&lt;/code&gt; is not strictly a dataframe (more of a promise to be a dataframe), so the only difference to fitting multiple non-linear regressions is the need to specify the &lt;code&gt;data&lt;/code&gt; using &lt;code&gt;dataframe(.)&lt;/code&gt; within &lt;strong&gt;map()&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After grouping the dataframe, new datasets are bootstrapped using &lt;strong&gt;modelr::bootstrap()&lt;/strong&gt;. Using &lt;strong&gt;unnest()&lt;/strong&gt; gives a new column called &lt;code&gt;boot_num&lt;/code&gt; which represents the bootstrap replicate within each group. The tibble is then re-grouped to include &lt;code&gt;boot_num&lt;/code&gt; and the model can finally be fitted to each bootstrapped dataset. Obviously the total number of models your code fits is increases as you up the number of bootstraps, so be aware this code may take a fair while to run!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# run nls.multstart on each curve of the original data ####
fit_many &amp;lt;- group_by(Chlorella_TRC, growth.temp, process, flux) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(., fit = purrr::map(data, ~nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                                   data = .x,
                                   iter = 500,
                                   param_bds = c(-100, 100, 0.1, 2, 0.5, 10, 285, 330),
                                   supp_errors = &amp;#39;Y&amp;#39;,
                                   AICc = &amp;#39;Y&amp;#39;,
                                   na.action = na.omit,
                                   lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))))

# run bootstrap over many curves ####
boot_many &amp;lt;- group_by(Chlorella_TRC, growth.temp, process, flux) %&amp;gt;%
  # create 200 bootstrap replicates per curve
  do(., boot = modelr::bootstrap(., n = 200, id = &amp;#39;boot_num&amp;#39;)) %&amp;gt;%
  # unnest to show bootstrap number, .id
  unnest() %&amp;gt;%
  # regroup to include the boot_num
  group_by(., growth.temp, process, flux, boot_num) %&amp;gt;%
  # run the model using map()
  mutate(fit = map(strap, ~nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                        data = data.frame(.),
                        iter = 50,
                        param_bds = c(-10, 10, 0.1, 2, 0.5, 5, 285, 330),
                        lower = c(lnc=-10, E=0, Eh=0, Th=0),
                        upper = c(lnc = 5, E = 10, Eh = 30, Th = 350),
                        supp_errors = &amp;#39;Y&amp;#39;)
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Smooth predictions can then be calculated from &lt;code&gt;fit_many&lt;/code&gt; and &lt;code&gt;boot_many&lt;/code&gt; and plotted to demonstrate the uncertainty of multiple curves. I do some wrangling to get the &lt;code&gt;max&lt;/code&gt; and &lt;code&gt;min&lt;/code&gt; temperature of each curve so that I only plot predictions over the range of each specific curve (some have measurements up to 46 ºC, some to 49 ºC).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new data frame for smooth predictions
new_preds &amp;lt;- Chlorella_TRC %&amp;gt;%
  do(., data.frame(K = seq(min(.$K), max(.$K), length.out = 150), stringsAsFactors = FALSE))

# get max and min for each curve
max_min &amp;lt;- group_by(Chlorella_TRC, growth.temp, flux, process) %&amp;gt;%
  summarise(., min_K = min(K), max_K = max(K)) %&amp;gt;%
  ungroup()

# create smoother predictions for unbootstrapped models
preds_many &amp;lt;- fit_many %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment, newdata = new_preds))

# create smoother predictions for bootstrapped replicates
preds_many &amp;lt;- boot_many %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment, newdata = new_preds)) %&amp;gt;%
  ungroup() %&amp;gt;%
  # group by each value of K and get quantiles
  group_by(., growth.temp, process, flux, K) %&amp;gt;%
  summarise(lwr_CI = quantile(.fitted, 0.025),
            upr_CI = quantile(.fitted, 0.975)) %&amp;gt;%
  ungroup() %&amp;gt;%
  merge(., preds_many, by = c(&amp;#39;K&amp;#39;, &amp;#39;growth.temp&amp;#39;, &amp;#39;flux&amp;#39;, &amp;#39;process&amp;#39;)) %&amp;gt;%
  # merge with max_min to delete predictions outside of the max and min temperatures of each curve
  merge(., max_min, by = c(&amp;#39;growth.temp&amp;#39;, &amp;#39;flux&amp;#39;, &amp;#39;process&amp;#39;)) %&amp;gt;%
  group_by(., growth.temp, flux, process) %&amp;gt;%
  filter(., K &amp;gt;= unique(min_K) &amp;amp; K &amp;lt;= unique(max_K)) %&amp;gt;%
  rename(., ln.rate = .fitted) %&amp;gt;%
  ungroup()

# plot predictions 
ggplot(Chlorella_TRC, aes(K - 273.15, ln.rate, group = flux)) +
  geom_point(alpha = 0.5, size = 0.5) +
  geom_line(data = preds_many) +
  geom_ribbon(aes(ymin = lwr_CI, ymax = upr_CI, fill = flux), data = preds_many, alpha = .2) + 
  scale_fill_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  facet_wrap(~ process + growth.temp, labeller = labeller(.multi_line = FALSE)) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  theme(legend.position = c(0.9, 0.15)) +
  ggtitle(&amp;#39;Multiple TPCs with confidence intervals&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-bootstrapping-non-linear-models-with-broom_files/figure-html/plot_many-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When in doubt, it seems that &lt;strong&gt;purrr&lt;/strong&gt; has the answer. Bootstrapping is a super useful method for visualising the uncertainty of predictions for non-linear regressions, and better allow us to understand how well a particular model fits the data. The &lt;strong&gt;tidyverse&lt;/strong&gt;, as usual, provides a set of tools that makes this method easy to understand and implement, sort of.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;Another way of visualising uncertainty in predictions is by using a Bayesian approach. When using Stan, the &lt;code&gt;generated quantities{}&lt;/code&gt; block is a great way to create new predictions from the data. This allows for models that can account for the structure of the data (i.e. temperatures within replicates) and visualise the uncertainty of the model fits. The package &lt;strong&gt;brms&lt;/strong&gt; looks to be a great help in fitting non-linear mixed models in a Bayesian framework. Just need to get round to experimenting with it!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting non-linear regressions with broom, purrr and nls.multstart</title>
      <link>/post/fitting-non-linear-regressions-with-broom-purrr-and-nls.multstart/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/fitting-non-linear-regressions-with-broom-purrr-and-nls.multstart/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;With my research, I often use non-linear least squares regression to fit a model with biologically meaningful parameters to data. Specifically, I measure the thermal performance of phytoplankon growth, respiration and photosynthesis over a wide range of assay temperatures to see how the organisms are adapted to the temperatures they live at.&lt;/p&gt;
&lt;p&gt;These thermal performance curves generally follow a unimodal shape and parameters for which are widely used in climate change research to predict whether organisms will be able to cope with increasing temperatures.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/TPC.png&#34; alt=&#34;Example Thermal Performance Curve&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Example Thermal Performance Curve&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These curves can be modelled with a variety of equations, such as the Sharpe-Schoolfield equation, which I have log-transformed here:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[log(rate) = lnc + E(\frac{1}{T_{c}} - \frac{1}{kT}) - ln(1 + e^{E_h(\frac{1}{kT_h} - \frac{1}{kT})})\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(lnc\)&lt;/span&gt; is a normalisation constant at a common temperature, &lt;span class=&#34;math inline&#34;&gt;\(T_{c}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; is an activation energy that describes the rate of increase before the optimum temperature, &lt;span class=&#34;math inline&#34;&gt;\(T_{opt}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is Boltzmann’s constant, &lt;span class=&#34;math inline&#34;&gt;\(E_{h}\)&lt;/span&gt; is the deactivation energy that controls the decline in rate past the optimum temperature and &lt;span class=&#34;math inline&#34;&gt;\(T_{h}\)&lt;/span&gt; is the temperature where, after the optimu, the rate is half of the maximal rate.&lt;/p&gt;
&lt;p&gt;Say I want to fit the same equation to 10, 50, or 100s of these curves. I could loop through a call to &lt;strong&gt;nls()&lt;/strong&gt;, &lt;strong&gt;nlsLM()&lt;/strong&gt;, or use &lt;strong&gt;nlsList()&lt;/strong&gt; from &lt;strong&gt;nlme&lt;/strong&gt;. However, non-linear least squares regression in R is sensitive to the start parameters, meaning that different start parameters can give different “best estimated parameters”. This becomes more likely when fitting more curves with only a single set of start parameters, where the variation in estimated parameter values is likely to be much larger. For example, some curves could have much higher rates (&lt;span class=&#34;math inline&#34;&gt;\(lnc\)&lt;/span&gt;), higher optimum temperatures (i.e. &lt;span class=&#34;math inline&#34;&gt;\(T_{h}\)&lt;/span&gt;) or have different values of temperature-dependence (&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To combat this, I wrote an R package which allows for multiple start parameters for non-linear regression. I wrapped this method in an R package called &lt;a href=&#34;https://github.com/padpadpadpad/nlsLoop&#34;&gt;&lt;strong&gt;nlsLoop&lt;/strong&gt;&lt;/a&gt; and submitted it to The Journal of Open Source Software. Everything was good with the world and I went to a Christmas party.&lt;/p&gt;
&lt;p&gt;The next day, I had an epiphany surrounding the redundancies and needless complexities of my R package, withdrew my submission and rewrote the entire package in a weekend to give rise to a single function package, &lt;strong&gt;nls.multstart::nls_multstart()&lt;/strong&gt;. Essentially since I first wrote &lt;strong&gt;nlsLoop&lt;/strong&gt; ~3 years ago I have realised that &lt;strong&gt;broom&lt;/strong&gt; and &lt;strong&gt;purrr&lt;/strong&gt; can do what I wrote clunkier functions to achieve. In contrast, &lt;a href=&#34;https://github.com/padpadpadpad/nls.multstart&#34;&gt;&lt;strong&gt;nls.multstart&lt;/strong&gt;&lt;/a&gt; works perfectly with the tools of the &lt;strong&gt;tidyverse&lt;/strong&gt; to fit multiple models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-model-fitting-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple model fitting in practice&lt;/h2&gt;
&lt;p&gt;Load in all packages that are used in this analysis. Packages can be installed from GitHub using &lt;strong&gt;devtools&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(nls.multstart) # devtools::install_github(&amp;#39;padpadpadpad/nls.multstart&amp;#39;)
library(ggplot2)
library(broom)
library(purrr)
library(dplyr)
library(tidyr)
library(nlstools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then load in the data and have a look at it using &lt;strong&gt;glimpse()&lt;/strong&gt;. Here we shall use a dataset of thermal performance curves of metabolism of &lt;strong&gt;Chlorella vulgaris&lt;/strong&gt; from Padfield &lt;strong&gt;et al.&lt;/strong&gt; 2016.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load in example data set
data(&amp;quot;Chlorella_TRC&amp;quot;)

glimpse(Chlorella_TRC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 649
## Variables: 7
## $ curve_id    &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,...
## $ growth.temp &amp;lt;dbl&amp;gt; 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20...
## $ process     &amp;lt;chr&amp;gt; &amp;quot;acclimation&amp;quot;, &amp;quot;acclimation&amp;quot;, &amp;quot;acclimation&amp;quot;, &amp;quot;accl...
## $ flux        &amp;lt;chr&amp;gt; &amp;quot;respiration&amp;quot;, &amp;quot;respiration&amp;quot;, &amp;quot;respiration&amp;quot;, &amp;quot;resp...
## $ temp        &amp;lt;dbl&amp;gt; 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 16...
## $ K           &amp;lt;dbl&amp;gt; 289.15, 292.15, 295.15, 298.15, 301.15, 304.15, 30...
## $ ln.rate     &amp;lt;dbl&amp;gt; -2.06257833, -1.32437939, -0.95416807, -0.79443675...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we define the Sharpe-Schoolfield equation discussed earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define the Sharpe-Schoolfield equation
schoolfield_high &amp;lt;- function(lnc, E, Eh, Th, temp, Tc) {
  Tc &amp;lt;- 273.15 + Tc
  k &amp;lt;- 8.62e-5
  boltzmann.term &amp;lt;- lnc + log(exp(E/k*(1/Tc - 1/temp)))
  inactivation.term &amp;lt;- log(1/(1 + exp(Eh/k*(1/Th - 1/temp))))
  return(boltzmann.term + inactivation.term)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 60 curves in this dataset, 30 each of photosynthesis and respiration. The treatments are growth temperature (20, 23, 27, 30, 33 ºC) and adaptive process (acclimation or adaptation) that reflects the number of generations cultures were grown at each temperature.&lt;/p&gt;
&lt;p&gt;We can see how &lt;strong&gt;nls_multstart()&lt;/strong&gt; works by subsetting the data for a single curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subset dataset
d_1 &amp;lt;- subset(Chlorella_TRC, curve_id == 1)

# run nls_multstart
fit &amp;lt;- nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                     data = d_1,
                     iter = 500,
                     param_bds = c(-10, 10, 0.1, 2, 0.5, 5, 285, 330),
                     supp_errors = &amp;#39;Y&amp;#39;,
                     AICc = &amp;#39;Y&amp;#39;,
                     na.action = na.omit,
                     lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))

fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nonlinear regression model
##   model: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)
##    data: data
##      lnc        E       Eh       Th 
##  -1.3462   0.9877   4.3326 312.1887 
##  residual sum-of-squares: 7.257
## 
## Number of iterations to convergence: 19 
## Achieved convergence tolerance: 1.49e-08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;nls_multstart()&lt;/strong&gt; allows boundaries for each parameter to be set. A uniform distribution between these values is created and start values for each iteration of the fitting process are then picked randomly. The function returns the best available model by picking the model with the lowest AIC/AICc score. Additional info on the function can be found &lt;a href=&#34;https://github.com/padpadpadpad/nls.multstart&#34;&gt;here&lt;/a&gt; or by typing &lt;code&gt;?nls_multstart&lt;/code&gt; into the R console.&lt;/p&gt;
&lt;p&gt;This fit can then be “tidied” in various ways using the R package &lt;strong&gt;broom&lt;/strong&gt;. Each different function in &lt;strong&gt;broom&lt;/strong&gt; returns a different set of information. &lt;strong&gt;tidy()&lt;/strong&gt; returns the estimated parameters, &lt;strong&gt;augment()&lt;/strong&gt; returns the predictions and &lt;strong&gt;glance()&lt;/strong&gt; returns information about the model such as the AIC score and whether the model has reached convergence. Confidence intervals of non-linear regression can also be estimated using &lt;strong&gt;nlstools::confint2()&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The amazing thing about these tools is the ease at which they can then be used on multiple curves at once, an approach Hadley Wickham has previously &lt;a href=&#34;https://blog.rstudio.com/2016/02/02/tidyr-0-4-0/&#34;&gt;written about&lt;/a&gt;. The approach nests the data based on grouping variables using &lt;strong&gt;nest()&lt;/strong&gt;, then creates a list column of the best fit for each curve using &lt;strong&gt;map()&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit over each set of groupings
fits &amp;lt;- Chlorella_TRC %&amp;gt;%
  group_by(., flux, growth.temp, process, curve_id) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(fit = purrr::map(data, ~ nls_multstart(ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20),
                                   data = .x,
                                   iter = 1000,
                                   param_bds = c(-100, 100, 0.1, 2, 0.5, 10, 285, 330),
                                   supp_errors = &amp;#39;Y&amp;#39;,
                                   AICc = &amp;#39;Y&amp;#39;,
                                   na.action = na.omit,
                                   lower = c(lnc = -10, E = 0, Eh = 0, Th = 0))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are confused, then you are not alone. This took me a long time to understand and I imagine there are still better ways for me to do it! However, to check it has worked, we can look at a single fit to check it looks ok. We can also look at &lt;code&gt;fits&lt;/code&gt; to see that there is now a &lt;code&gt;fit&lt;/code&gt; list column containing each of the non-linear fits for each combination of our grouping variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at a single fit
summary(fits$fit[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: ln.rate ~ schoolfield_high(lnc, E, Eh, Th, temp = K, Tc = 20)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## lnc  -1.3462     0.4656  -2.891   0.0202 *  
## E     0.9877     0.4521   2.185   0.0604 .  
## Eh    4.3326     1.4878   2.912   0.0195 *  
## Th  312.1887     3.8782  80.499 6.32e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9524 on 8 degrees of freedom
## 
## Number of iterations to convergence: 16 
## Achieved convergence tolerance: 1.49e-08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at output object
select(fits, curve_id, data, fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 x 3
##    curve_id data              fit      
##       &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;   
##  1     1.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  2     2.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  3     3.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  4     4.00 &amp;lt;tibble [9 × 3]&amp;gt;  &amp;lt;S3: nls&amp;gt;
##  5     5.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  6     6.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  7     7.00 &amp;lt;tibble [12 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  8     8.00 &amp;lt;tibble [10 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
##  9     9.00 &amp;lt;tibble [8 × 3]&amp;gt;  &amp;lt;S3: nls&amp;gt;
## 10    10.0  &amp;lt;tibble [10 × 3]&amp;gt; &amp;lt;S3: nls&amp;gt;
## # ... with 50 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These fits can be cleaned up using the &lt;strong&gt;broom&lt;/strong&gt; functions and &lt;strong&gt;purrr::map()&lt;/strong&gt; to iterate over the grouping variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get summary info
info &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(glance))

# get params
params &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(tidy))

# get confidence intervals
CI &amp;lt;- fits %&amp;gt;% 
  unnest(fit %&amp;gt;% map(~ confint2(.x) %&amp;gt;%
  data.frame() %&amp;gt;%
  rename(., conf.low = X2.5.., conf.high = X97.5..))) %&amp;gt;%
  group_by(., curve_id) %&amp;gt;%
  mutate(., term = c(&amp;#39;lnc&amp;#39;, &amp;#39;E&amp;#39;, &amp;#39;Eh&amp;#39;, &amp;#39;Th&amp;#39;)) %&amp;gt;%
  ungroup()

# merge parameters and CI estimates
params &amp;lt;- merge(params, CI, by = intersect(names(params), names(CI)))

# get predictions
preds &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at &lt;strong&gt;info&lt;/strong&gt; allows us to see if all the models converged.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(info, curve_id, logLik, AIC, BIC, deviance, df.residual)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 x 6
##    curve_id  logLik   AIC   BIC deviance df.residual
##       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;int&amp;gt;
##  1     1.00 -14.0   38.0  40.4     7.26            8
##  2     2.00 - 1.20  12.4  14.8     0.858           8
##  3     3.00 - 7.39  24.8  27.2     2.41            8
##  4     4.00 - 0.523 11.0  12.0     0.592           5
##  5     5.00 -10.8   31.7  34.1     4.29            8
##  6     6.00 - 8.52  27.0  29.5     2.91            8
##  7     7.00 - 1.29  12.6  15.0     0.871           8
##  8     8.00 -13.4   36.7  38.2     8.48            6
##  9     9.00   1.82   6.36  6.76    0.297           4
## 10    10.0  - 1.27  12.5  14.1     0.755           6
## # ... with 50 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When plotting non-linear fits, I prefer to have a smooth curve, even when there are not many points underlying the fit. This can be achieved by including &lt;code&gt;newdata&lt;/code&gt; in the &lt;strong&gt;augment()&lt;/strong&gt; function and creating a higher resolution set of predictor values.&lt;/p&gt;
&lt;p&gt;However, when predicting for many different fits, it is not certain that each curve has the same range of predictor variables. We can get around this by setting the limits of each prediction by the &lt;strong&gt;min()&lt;/strong&gt; and &lt;strong&gt;max()&lt;/strong&gt; of the predictor variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new data frame of predictions
new_preds &amp;lt;- Chlorella_TRC %&amp;gt;%
  do(., data.frame(K = seq(min(.$K), max(.$K), length.out = 150), stringsAsFactors = FALSE))

# max and min for each curve
max_min &amp;lt;- group_by(Chlorella_TRC, curve_id) %&amp;gt;%
  summarise(., min_K = min(K), max_K = max(K)) %&amp;gt;%
  ungroup()

# create new predictions
preds2 &amp;lt;- fits %&amp;gt;%
  unnest(fit %&amp;gt;% map(augment, newdata = new_preds)) %&amp;gt;%
  merge(., max_min, by = &amp;#39;curve_id&amp;#39;) %&amp;gt;%
  group_by(., curve_id) %&amp;gt;%
  filter(., K &amp;gt; unique(min_K) &amp;amp; K &amp;lt; unique(max_K)) %&amp;gt;%
  rename(., ln.rate = .fitted) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These can then be plotted using &lt;strong&gt;ggplot2&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot
ggplot() +
  geom_point(aes(K - 273.15, ln.rate, col = flux), size = 2, Chlorella_TRC) +
  geom_line(aes(K - 273.15, ln.rate, col = flux, group = curve_id), alpha = 0.5, preds2) +
  facet_wrap(~ growth.temp + process, labeller = labeller(.multi_line = FALSE)) +
  scale_colour_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  ylab(&amp;#39;log Metabolic rate&amp;#39;) +
  xlab(&amp;#39;Assay temperature (ºC)&amp;#39;) +
  theme(legend.position = c(0.9, 0.15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-fitting-many-non-linear-regressions-with-broom-purrr-and-nls-multstart_files/figure-html/plot_many_fits-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The confidence intervals of each parameter for each curve fit can also be easily visualised.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot
ggplot(params, aes(col = flux)) +
  geom_point(aes(curve_id, estimate)) +
  facet_wrap(~ term, scale = &amp;#39;free_x&amp;#39;, ncol = 4) +
  geom_linerange(aes(curve_id, ymin = conf.low, ymax = conf.high)) +
  coord_flip() +
  scale_color_manual(values = c(&amp;#39;green4&amp;#39;, &amp;#39;black&amp;#39;)) +
  theme_bw(base_size = 12, base_family = &amp;#39;Helvetica&amp;#39;) +
  theme(legend.position = &amp;#39;top&amp;#39;) +
  xlab(&amp;#39;curve&amp;#39;) +
  ylab(&amp;#39;parameter estimate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-22-fitting-many-non-linear-regressions-with-broom-purrr-and-nls-multstart_files/figure-html/confint_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This method of modelling can be used for different data, different non-linear models (and linear models for that matter) and combined with the &lt;strong&gt;tidyverse&lt;/strong&gt; can make very useful visualisations.&lt;/p&gt;
&lt;p&gt;The next stage of these curve fits is to try and better understand the uncertainty of these curve fits and their predictions. One approach to achieve this could be bootstrapping new datasets from the existing data. I hope to demonstrate how this could be done soon in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Padfield, D., Yvon-durocher, G., Buckling, A., Jennings, S. &amp;amp; Yvon-durocher, G. (2016). Rapid evolution of metabolic traits explains thermal adaptation in phytoplankton. Ecology Letters, 19(2), 133-142.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
